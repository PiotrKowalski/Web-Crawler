from bs4 import BeautifulSoup
from urllib.request import urlopen
import sys
import json
from urllib.parse import urljoin
import urllib.request
import time
"""
If there is a problem with recursion limit, set a new limit
Extensions are to prevent the program from entering not. html files so as to it won't return errors
To crawl through websites, enter a index page url
Examples of websites are given below
"""
sys.setrecursionlimit(1000)

EXTENSIONS = ('.txt', '.jpg', '.png', '.pdf')

# url = "https://www.pythonforbeginners.com/"
# url = "http://127.0.0.1:8000/"
url = "https://halome.nu/"
# url = "http://quirktools.com/screenfly/"
# url = "http://www.smallwebsites.co/"


def site_map(url):
    """
    Saves site map to the .json file
    :param url: Url of index page of given website
    :return:
    """
    list_of_urls = []
    start_time = time.time()

    list_of_urls = get_urls(url, list_of_urls, url)

    result = get_titles_and_links(list_of_urls, url)

    # print(result)
    print(json.dumps(result, indent=2))
    print("--- %s seconds ---" % (time.time() - start_time))

    with open(r'data.json', 'w') as outfile:
        json.dump(result, outfile)


def get_urls(url, list_of_urls, main_url):
    """
    Getting data of all accessible urls
    :param url: Mutable Url used in finding next urls by recursion method
    :param list_of_urls: Empty list which is being filled with urls while the function is working
    :param main_url: Immutable url used in verification of the beginning of links
    :return: Filled list list_of_urls with every accessible link
    """
    if url in list_of_urls:
        return list_of_urls

    try:
        content = urlopen(url).read()
        soup = BeautifulSoup(content, "html.parser")
        list_of_urls.append(url)

        links = [urljoin(url, link.get('href')) for link in soup.find_all('a') if not urljoin(url,
                                                                                              link.get('href')).endswith(EXTENSIONS)]
        for link in links:
            if link.startswith(main_url) and link not in list_of_urls:
                list_of_urls = get_urls(link, list_of_urls, main_url)

    except urllib.error.HTTPError or ValueError:
        pass

    return list_of_urls


def get_titles_and_links(list_of_urls, main_url):
    """
    This function generates titles and urls to associated urls and adds it to url value in dictionary
        Example of one element generated by one url:
        {
        url: {
        'title': title,
        'links': [list of links]
            }
        }
    :param list_of_urls: Filled list with urls based on which title and links are fetched from associated website
    :param main_url: Immutable url used in verification of the beginning of links
    :return: Returns n-element dictionary. n is equal to length of list_of_urls list
    """
    target = {url: {} for url in list_of_urls}
    for url in list_of_urls:
        content = urlopen(url).read()

        soup = BeautifulSoup(content, "html.parser")

        title = soup.title.string
        if title:
            target[url] = {'title': title}
        else:
            target[url] = {'title': 'No links here'}

        # I have to make list because .json file can't recognise sets and json.dump() doesn't work.

        links = list({urljoin(url, link.get('href')) for link in soup.find_all('a') if main_url in urljoin(url, link.get('href'))})

        if not links:
            target[url]['links'] = 'set()'
        else:
            target[url]['links'] = links

    return target


# site_map(url)
